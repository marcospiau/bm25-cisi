{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict, namedtuple\n",
    "from pathlib import Path\n",
    "import toolz\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import more_itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dos dados\n",
    "\n",
    "Para fazer download dos dados, vá até a pasta raiz e digite:\n",
    "```bash\n",
    "make get_raw_data\n",
    "\n",
    "```\n",
    "Para gerar esse Makefile, eu dei a seguinte query ao chat GPT:\n",
    "```\n",
    "write a makefile to do the following:\n",
    "- download file from http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\n",
    "- unzip it to the folder data/raw\n",
    "- it should be on step \"get_raw_data\"\n",
    "- operation should not be repeated if files already exist\n",
    "- write help for the steps\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAW_DATA_BASEDIR = Path('../data/raw')\n",
    "!ls -lht {RAW_DATA_BASEDIR}\n",
    "\n",
    "raw_files_path = {\n",
    "    'query': RAW_DATA_BASEDIR / 'CISI.QRY',\n",
    "    'all': RAW_DATA_BASEDIR / 'CISI.ALL',\n",
    "    'rel': RAW_DATA_BASEDIR / 'CISI.REL',\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversão dos dados para um formato mais fácil de trabalhar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6200\n",
      "-rw-r--r--  1 marcospiau  staff   757K Feb 22 19:27 cisi.tar.gz\n",
      "-rw-r--r--  1 marcospiau  staff    79K Feb 28  1994 CISI.REL\n",
      "-rw-r--r--  1 marcospiau  staff    67K Feb 28  1994 CISI.QRY\n",
      "-rw-r--r--  1 marcospiau  staff   4.5K Feb 28  1994 CISI.BLN\n",
      "-rw-r--r--  1 marcospiau  staff   2.1M Feb 28  1994 CISI.ALL\n"
     ]
    }
   ],
   "source": [
    "RAW_DATA_BASEDIR = Path('../data/raw')\n",
    "!ls -lht {RAW_DATA_BASEDIR}\n",
    "\n",
    "raw_files_path = {\n",
    "    'query': RAW_DATA_BASEDIR / 'CISI.QRY',\n",
    "    'all': RAW_DATA_BASEDIR / 'CISI.ALL',\n",
    "    'rel': RAW_DATA_BASEDIR / 'CISI.REL',\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquivos QRY e ALL são mais complicados e precisam de processamento. Tentei deixar processamento semelhante pra conseguir fazer os dois casos com uma mesma funcao.\n",
    "\n",
    "Usei bastante o chatgpt, principalmente para escrever docstrings e typehints. Uma coisa curiosa é que constantemente ele removia a dataclass e utilizava classes padrão, de forma que precisei constamente instruir ele a a manter as dataclasses.\n",
    "\n",
    "Utilizei bastante a bibliioteca toolz para deixar o código com uma abordagem mais funcional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (.I) ID\n",
    "# (.T) Title\n",
    "# (.W) Abstract\n",
    "# (.B) Publication date of the article\n",
    "# (.A) Author list\n",
    "# (.N) Information when entry was added\n",
    "# (.X) List of cross-references to other documents\n",
    "\n",
    "renames_docs_all = {\n",
    "    '.I': 'id',\n",
    "    '.T': 'title',\n",
    "    '.W': 'abstract',\n",
    "    '.B': 'publication_date',\n",
    "    '.A': 'author_list',\n",
    "    '.N': 'added_date',\n",
    "    '.X': 'cross_references'\n",
    "}\n",
    "\n",
    "process_doc_all = toolz.compose_left(\n",
    "    # for each tag, get renamed tag, and join texts for tag\n",
    "    functools.partial(more_itertools.map_reduce,\n",
    "                      keyfunc=lambda x: renames_docs_all[x.tag],\n",
    "                      valuefunc=lambda x: x.text,\n",
    "                      reducefunc=lambda x: ' '.join(x)),\n",
    "    # keeps only desired keys\n",
    "    toolz.curried.keyfilter(lambda x: x in {'id', 'title', 'abstract'}),\n",
    "    # convert id to int\n",
    "    toolz.curried.update_in(keys=['id'], func=int)\n",
    ")\n",
    "\n",
    "# (.I) ID\n",
    "# (.W) Query\n",
    "# (.A) Author list\n",
    "# (.N) Authors name and some keywords on what the query searches for\n",
    "\n",
    "renames_docs_queries = {\n",
    "    '.I': 'id',\n",
    "    '.W': 'query',\n",
    "    '.T': 'title',\n",
    "    '.A': 'author_list',\n",
    "    # '.N': 'other_query_infos',\n",
    "    '.B': 'publication_date',\n",
    "    # '.X': 'cross_references'\n",
    "}\n",
    "\n",
    "process_doc_qry = toolz.compose_left(\n",
    "    functools.partial(more_itertools.map_reduce,\n",
    "                      keyfunc=lambda x: renames_docs_queries[x.tag],\n",
    "                      valuefunc=lambda x: x.text,\n",
    "                      reducefunc=lambda x: ' '.join(x)),\n",
    "    toolz.curried.keyfilter(lambda x: x in {'id', 'query'}),\n",
    "    toolz.curried.update_in(keys=['id'], func=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "from typing import Any, Callable, Dict, Iterable, List, Tuple, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IdTagText:\n",
    "    id: str\n",
    "    tag: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "def parse_cisi_all_or_qry(\n",
    "    path: str,\n",
    "    process_doc_fn: Callable[[Iterable[IdTagText]], Dict[str, Any]],\n",
    "    return_dict: bool = False,\n",
    ") -> Union[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"Parses a CISI all or query file.\n",
    "\n",
    "    Args:\n",
    "        path: A string representing the path to the file to parse.\n",
    "        process_doc_fn: A function that receives a list of IdTagText or an iterable of\n",
    "          IdTagText as input, and returns a dictionary.\n",
    "        return_dict: A boolean indicating whether to return a dictionary or a list of\n",
    "          dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        If return_dict is True, a dictionary with the document IDs as keys and\n",
    "        processed documents as values. Otherwise, a list of processed documents.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the document IDs are not unique.\n",
    "    \"\"\"\n",
    "    markers = [r'^\\.I\\s(\\d+)$', '\\.T$', '\\.A$', '\\.B$', '\\.W$', '\\.X$', '\\.N$']\n",
    "    marker_pattern = re.compile('|'.join(markers))\n",
    "\n",
    "    def gen_items() -> Iterable[IdTagText]:\n",
    "        with open(path, 'r') as f:\n",
    "            for line in map(str.strip, f):\n",
    "                match = marker_pattern.match(line)\n",
    "                # match occurs for lines with tags\n",
    "                if match:\n",
    "                    # if match.group(1) is not None, it means that the tag is .I\n",
    "                    # and the group contains the ID\n",
    "                    if match.group(1):\n",
    "                        id_ = match.group(1)\n",
    "                        yield IdTagText(id_, '.I', id_)\n",
    "                    else:\n",
    "                        tag = match.group(0).strip()\n",
    "                else:\n",
    "                    # if match is None, it means that the line is a text\n",
    "                    # just propagate tag and id\n",
    "                    yield IdTagText(id_, tag, line)\n",
    "\n",
    "    out = [\n",
    "        process_doc_fn(group)\n",
    "        for _, group in itertools.groupby(gen_items(), key=lambda x: x.id)\n",
    "    ]\n",
    "    assert len(out) == len({x['id'] for x in out}), 'IDs are not unique'\n",
    "    return {x['id']: x for x in out} if return_dict else out\n",
    "\n",
    "\n",
    "process_cisi_all = functools.partial(parse_cisi_all_or_qry,\n",
    "                                     process_doc_fn=process_doc_all)\n",
    "process_cisi_qry = functools.partial(parse_cisi_all_or_qry,\n",
    "                                     process_doc_fn=process_doc_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'title': '18 Editions of the Dewey Decimal Classifications',\n",
       " 'abstract': \"The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = process_cisi_all(raw_files_path['all'])\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112,\n",
       " {'id': 1,\n",
       "  'query': 'What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?'})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = process_cisi_qry(raw_files_path['query'])\n",
    "len(queries), queries[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O arquivo qrels é mais simples e pode ser lido de forma mais simples. O resultado é um dicionário com a query como chave e uma lista de documentos relevantes para aquela query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {28,\n",
       "   35,\n",
       "   38,\n",
       "   42,\n",
       "   43,\n",
       "   52,\n",
       "   65,\n",
       "   76,\n",
       "   86,\n",
       "   150,\n",
       "   189,\n",
       "   192,\n",
       "   193,\n",
       "   195,\n",
       "   215,\n",
       "   269,\n",
       "   291,\n",
       "   320,\n",
       "   429,\n",
       "   465,\n",
       "   466,\n",
       "   482,\n",
       "   483,\n",
       "   510,\n",
       "   524,\n",
       "   541,\n",
       "   576,\n",
       "   582,\n",
       "   589,\n",
       "   603,\n",
       "   650,\n",
       "   680,\n",
       "   711,\n",
       "   722,\n",
       "   726,\n",
       "   783,\n",
       "   813,\n",
       "   820,\n",
       "   868,\n",
       "   869,\n",
       "   894,\n",
       "   1162,\n",
       "   1164,\n",
       "   1195,\n",
       "   1196,\n",
       "   1281}),\n",
       " (2,\n",
       "  {29,\n",
       "   68,\n",
       "   197,\n",
       "   213,\n",
       "   214,\n",
       "   309,\n",
       "   319,\n",
       "   324,\n",
       "   429,\n",
       "   499,\n",
       "   636,\n",
       "   669,\n",
       "   670,\n",
       "   674,\n",
       "   690,\n",
       "   692,\n",
       "   695,\n",
       "   700,\n",
       "   704,\n",
       "   709,\n",
       "   720,\n",
       "   731,\n",
       "   733,\n",
       "   738,\n",
       "   740,\n",
       "   1136}),\n",
       " (3,\n",
       "  {60,\n",
       "   85,\n",
       "   114,\n",
       "   123,\n",
       "   126,\n",
       "   131,\n",
       "   133,\n",
       "   136,\n",
       "   138,\n",
       "   140,\n",
       "   346,\n",
       "   359,\n",
       "   363,\n",
       "   372,\n",
       "   412,\n",
       "   445,\n",
       "   454,\n",
       "   461,\n",
       "   463,\n",
       "   469,\n",
       "   532,\n",
       "   537,\n",
       "   540,\n",
       "   553,\n",
       "   554,\n",
       "   555,\n",
       "   585,\n",
       "   590,\n",
       "   599,\n",
       "   640,\n",
       "   660,\n",
       "   664,\n",
       "   803,\n",
       "   901,\n",
       "   909,\n",
       "   911,\n",
       "   1027,\n",
       "   1053,\n",
       "   1169,\n",
       "   1179,\n",
       "   1181,\n",
       "   1190,\n",
       "   1191,\n",
       "   1326}),\n",
       " (4, {310, 315, 321, 329, 332, 420, 601, 980}),\n",
       " (5,\n",
       "  {32,\n",
       "   56,\n",
       "   65,\n",
       "   114,\n",
       "   124,\n",
       "   137,\n",
       "   187,\n",
       "   188,\n",
       "   191,\n",
       "   241,\n",
       "   339,\n",
       "   370,\n",
       "   400,\n",
       "   451,\n",
       "   453,\n",
       "   471,\n",
       "   525,\n",
       "   528,\n",
       "   642,\n",
       "   648,\n",
       "   692,\n",
       "   1035,\n",
       "   1246,\n",
       "   1356})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_qrels(path):\n",
    "    out = defaultdict(set)\n",
    "    with open(path, 'r') as f:\n",
    "        for line in map(str.strip, f):\n",
    "            qid, docid, _, _ = line.split()\n",
    "            out[int(qid)].add(int(docid))\n",
    "    out.default_factory = None\n",
    "\n",
    "    return out\n",
    "\n",
    "qrels = load_qrels(raw_files_path['rel'])\n",
    "list(itertools.islice(qrels.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76,\n",
       " {'id': 1,\n",
       "  'query': 'What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?'})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtrando queries que aparecem no qrels:\n",
    "queries = [x for x in queries if x['id'] in qrels.keys()]\n",
    "len(queries), queries[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, vemos que todos os qrels possuem relevância igual a zero. Por isso, vamos interpretar que caso um documento esteja relacionado a uma query, ele é relevante para essa query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>nao</th>\n",
       "      <th>sei</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3114</td>\n",
       "      <td>3114</td>\n",
       "      <td>3114</td>\n",
       "      <td>3114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>76</td>\n",
       "      <td>1162</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>44</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>155</td>\n",
       "      <td>15</td>\n",
       "      <td>3114</td>\n",
       "      <td>3114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid docid   nao   sei\n",
       "count   3114  3114  3114  3114\n",
       "unique    76  1162     1     1\n",
       "top       44   375     0   0.0\n",
       "freq     155    15  3114  3114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(raw_files_path['rel'], delim_whitespace=True, header=None, names=['qid', 'docid', 'nao', 'sei']).astype(str)\n",
    "display(df.describe())\n",
    "del df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe BM25\n",
    "\n",
    "Utilizei bastante o ChatGPT para desevolver essa classe, comecei com uma classe mais simples (loops for) e fui modificando para conseguir deixar um pouco mais vetorizado. Estamos usando o CountVectorizer para facilitar a tokenização dos documentos e queries, o que também ajuda pois estão incluídos os stop words.\n",
    "\n",
    "Novamente, docstrings e typehints foram gerados pelo chatgpt.\n",
    "\n",
    "Utilizamos numpy e matrizes esparsas scipy para facilitar vetorização e operações matriciais.\n",
    "\n",
    "Valores default de k1 e b foram escolhidos pelo ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class BM25:\n",
    "\n",
    "    def __init__(self, corpus: List[dict], k1: float = 1.2, b: float = 0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "\n",
    "        self.corpus = {doc['id']: doc['text'] for doc in corpus}\n",
    "        self.docids_to_idx = {\n",
    "            docid: idx\n",
    "            for idx, docid in enumerate(self.corpus.keys())\n",
    "        }\n",
    "\n",
    "        # self.vectorizer = CountVectorizer(lowercase=True,\n",
    "        #                                   max_features=None,\n",
    "        #                                   stop_words=None,\n",
    "        #                                   tokenizer=self.tokenize,\n",
    "        #                                   preprocessor=lambda x: x)\n",
    "        self.vectorizer = CountVectorizer(lowercase=True,\n",
    "                                          max_features=None,\n",
    "                                          stop_words='english')\n",
    "        self.vectorizer.fit(self.corpus.values())\n",
    "        self.doc_term_matrix = self.vectorizer.transform(self.corpus.values())\n",
    "        assert self.doc_term_matrix.has_sorted_indices\n",
    "        self.corpus_lengths = self.doc_term_matrix.sum(axis=1).A1\n",
    "        self.avgdl = self.corpus_lengths.mean()\n",
    "        self.idf = self._calculate_idf()\n",
    "        # self.idf = self._calculate_idf()\n",
    "        # self.avgdl = sum(map(len, (doc['text'] for doc in self.corpus))) / len(self.corpus)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        raise NotImplementedError\n",
    "        # return re.sub(r'[^\\w\\s]', '', text).lower().split()\n",
    "\n",
    "\n",
    "    def _calculate_idf(self) -> np.ndarray:\n",
    "        \"\"\"Calculate the inverse document frequency for each term in the corpus using the BM25 formula.\"\"\"\n",
    "        # documents with term\n",
    "        df = self.doc_term_matrix.getnnz(axis=0)\n",
    "        N = self.doc_term_matrix.shape[0]\n",
    "        idf = np.log((N - df + 0.5) / (df + 0.5))\n",
    "        return idf\n",
    "\n",
    "    def score(self, query: str, doc_id: int) -> float:\n",
    "        \"\"\"Calculate the relevance score of a document for a given query.\n",
    "\n",
    "        Args:\n",
    "            query: Query text.\n",
    "            doc_id: ID of the document in the corpus.\n",
    "\n",
    "        Returns:\n",
    "            Relevance score of the document for the query.\n",
    "        \"\"\"\n",
    "        doc_tf = self.doc_term_matrix.getrow(self.docids_to_idx[doc_id])\n",
    "        assert doc_tf.has_sorted_indices\n",
    "        # doc_length = doc_tf.sum()\n",
    "        doc_length = self.corpus_lengths[self.docids_to_idx[doc_id]]\n",
    "        query_tf = self.vectorizer.transform([query])[0]\n",
    "        score = 0\n",
    "        for weight, term in zip(query_tf.data, query_tf.indices):\n",
    "            idf = self.idf[term]\n",
    "            tf = doc_tf[0, term]\n",
    "            score += idf * weight * (self.k1 + 1) / (\n",
    "                tf + self.k1 * (1 - self.b + self.b * doc_length / self.avgdl))\n",
    "        return score\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 10) -> List[dict]:\n",
    "        \"\"\"Retrieve the top-k documents for a given query.\n",
    "\n",
    "        Args:\n",
    "            query: Query text.\n",
    "            k: Number of documents to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            List of document dictionaries with 'id' and 'score' keys.\n",
    "        \"\"\"\n",
    "        scores = [\n",
    "            self.score(query, doc_id)\n",
    "            for doc_id in self.corpus.keys()\n",
    "        ]\n",
    "        docids = np.argsort(scores)[::-1][:k]\n",
    "        return [\n",
    "            {'id': docid, 'score': scores[docid]}\n",
    "            for docid in docids\n",
    "        ]\n",
    "\n",
    "    def get_results_for_all_queries(self, queries: List[dict], k: int = 10) -> Dict[int, List[dict]]:\n",
    "        \"\"\"Retrieve the top-k documents for all queries.\n",
    "\n",
    "        Args:\n",
    "            queries: List of query dictionaries with 'id' and 'text' keys.\n",
    "            k: Number of documents to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping query IDs to a list of document dictionaries with 'id' and 'score' keys.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            query['id']: self.retrieve(query['query'], k)\n",
    "            for query in tqdm(queries, desc='Retrieving')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_abstracts = [{'id': x['id'], 'text': x['abstract']} for x in corpus]\n",
    "bm25 = BM25(corpus=corpus_abstracts)\n",
    "# bm25 = BM25(corpus=corpus_abstracts, k1=0.9, b = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175.0623179775004"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.score(query=corpus[0]['abstract'], doc_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'title': '18 Editions of the Dewey Decimal Classifications',\n",
       " 'abstract': \"The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad.\"}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  86,  146,  388,  810, 1012, 1244, 1361, 1721, 2195, 2337, 2488,\n",
       "       2521, 2707, 2750, 3090, 3091, 3121, 3944, 4114, 4194, 4262, 5170,\n",
       "       5179, 5260, 5803, 6751, 6986, 7015, 8217, 8234, 8330, 8376, 8810,\n",
       "       9466], dtype=int32)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.doc_term_matrix[0].indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1295, 'score': 1451.6399967907266},\n",
       " {'id': 1287, 'score': 1451.6399967907266},\n",
       " {'id': 1283, 'score': 1389.7391687057677},\n",
       " {'id': 1311, 'score': 1345.7364667312902},\n",
       " {'id': 1085, 'score': 1345.7364667312902},\n",
       " {'id': 1301, 'score': 1338.2313415639899},\n",
       " {'id': 1288, 'score': 1335.856784250205},\n",
       " {'id': 930, 'score': 1333.4567012585728},\n",
       " {'id': 1305, 'score': 1298.3753665401923},\n",
       " {'id': 1278, 'score': 1298.3753665401923}]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.retrieve(query=corpus[1]['abstract'], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1287, 'score': 366.937721998151},\n",
       " {'id': 1295, 'score': 366.937721998151},\n",
       " {'id': 1283, 'score': 353.04613444162817},\n",
       " {'id': 1085, 'score': 340.1679993689292},\n",
       " {'id': 1301, 'score': 340.1679993689292},\n",
       " {'id': 1288, 'score': 340.1679993689292},\n",
       " {'id': 930, 'score': 340.1679993689292},\n",
       " {'id': 1311, 'score': 340.1679993689292},\n",
       " {'id': 1300, 'score': 328.1963161321295},\n",
       " {'id': 1278, 'score': 328.1963161321295}]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.retrieve(query=queries[0]['query'], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving: 100%|██████████| 76/76 [00:36<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "results_top10 = bm25.get_results_for_all_queries(queries, k=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliacao das métricas\n",
    "\n",
    "Para facilitar avaliacão das métricas e ter um script confiável, vamos utilizar o script trec_eval. Ele está instalado na pasta `bin`, dentro da raiz do projeto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertendo dados para format trec_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QRELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 1281 1\n",
      "1 0 650 1\n",
      "1 0 1162 1\n",
      "1 0 524 1\n",
      "1 0 269 1\n",
      "1 0 1164 1\n",
      "1 0 783 1\n",
      "1 0 894 1\n",
      "1 0 150 1\n",
      "1 0 28 1\n"
     ]
    }
   ],
   "source": [
    "from typing import Set\n",
    "\n",
    "def convert_qrels_to_trec_format(qrels: Dict[int, Set[int]], output_path: str):\n",
    "    \"\"\"Convert qrels to TREC format.\n",
    "\n",
    "    Args:\n",
    "        qrels: Dictionary of qrels.\n",
    "        output_path: Path to the output file.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        for qid, docids in qrels.items():\n",
    "            for docid in docids:\n",
    "                f.write(f'{qid} 0 {docid} 1\\n')\n",
    "\n",
    "!mkdir -pv '../data/processed/cisi'\n",
    "convert_qrels_to_trec_format(qrels, '../data/processed/cisi/qrels.txt')\n",
    "!head '../data/processed/cisi/qrels.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Q0 1287 1 366.937721998151 RUN\n",
      "1 Q0 1295 2 366.937721998151 RUN\n",
      "1 Q0 1283 3 353.04613444162817 RUN\n",
      "1 Q0 1085 4 340.1679993689292 RUN\n",
      "1 Q0 1301 5 340.1679993689292 RUN\n",
      "1 Q0 1288 6 340.1679993689292 RUN\n",
      "1 Q0 930 7 340.1679993689292 RUN\n",
      "1 Q0 1311 8 340.1679993689292 RUN\n",
      "1 Q0 1300 9 328.1963161321295 RUN\n",
      "1 Q0 1278 10 328.1963161321295 RUN\n"
     ]
    }
   ],
   "source": [
    "def convert_results_to_trec_eval_format(results: Dict[str, List[str]], output_path: str):\n",
    "    \"\"\"Converts results to trec_eval format.\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary of results.\n",
    "        output_path: Path to the output file.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        for query_id, docs in results.items():\n",
    "            for rank, doc in enumerate(docs, start=1):\n",
    "                doc_id = doc['id']\n",
    "                score = doc['score']\n",
    "                line = f\"{query_id} Q0 {doc_id} {rank} {score} RUN\\n\"\n",
    "                f.write(line)\n",
    "\n",
    "convert_results_to_trec_eval_format(results_top10, '../data/processed/cisi/results_abstract_top10.txt')\n",
    "!head '../data/processed/cisi/results_abstract_top10.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\tRUN\n",
      "num_q                 \tall\t76\n",
      "num_ret               \tall\t76000\n",
      "num_rel               \tall\t3114\n",
      "num_rel_ret           \tall\t2091\n",
      "map                   \tall\t0.0228\n",
      "gm_map                \tall\t0.0150\n",
      "Rprec                 \tall\t0.0244\n",
      "bpref                 \tall\t0.6718\n",
      "recip_rank            \tall\t0.0695\n",
      "iprec_at_recall_0.00  \tall\t0.0848\n",
      "iprec_at_recall_0.10  \tall\t0.0508\n",
      "iprec_at_recall_0.20  \tall\t0.0353\n",
      "iprec_at_recall_0.30  \tall\t0.0332\n",
      "iprec_at_recall_0.40  \tall\t0.0312\n",
      "iprec_at_recall_0.50  \tall\t0.0298\n",
      "iprec_at_recall_0.60  \tall\t0.0255\n",
      "iprec_at_recall_0.70  \tall\t0.0109\n",
      "iprec_at_recall_0.80  \tall\t0.0014\n",
      "iprec_at_recall_0.90  \tall\t0.0003\n",
      "iprec_at_recall_1.00  \tall\t0.0001\n",
      "P_5                   \tall\t0.0211\n",
      "P_10                  \tall\t0.0237\n",
      "P_15                  \tall\t0.0237\n",
      "P_20                  \tall\t0.0237\n",
      "P_30                  \tall\t0.0289\n",
      "P_100                 \tall\t0.0311\n",
      "P_200                 \tall\t0.0271\n",
      "P_500                 \tall\t0.0280\n",
      "P_1000                \tall\t0.0275\n"
     ]
    }
   ],
   "source": [
    "!../bin/trec_eval-9.0.7/trec_eval ../data/processed/cisi/qrels.txt ../data/processed/cisi/results_abstract_top10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\tRUN\n",
      "num_q                 \tall\t76\n",
      "num_ret               \tall\t2091\n",
      "num_rel               \tall\t3114\n",
      "num_rel_ret           \tall\t2091\n",
      "map                   \tall\t0.6718\n",
      "gm_map                \tall\t0.6601\n",
      "Rprec                 \tall\t0.6718\n",
      "bpref                 \tall\t0.6718\n",
      "recip_rank            \tall\t1.0000\n",
      "iprec_at_recall_0.00  \tall\t1.0000\n",
      "iprec_at_recall_0.10  \tall\t1.0000\n",
      "iprec_at_recall_0.20  \tall\t1.0000\n",
      "iprec_at_recall_0.30  \tall\t1.0000\n",
      "iprec_at_recall_0.40  \tall\t0.9737\n",
      "iprec_at_recall_0.50  \tall\t0.9211\n",
      "iprec_at_recall_0.60  \tall\t0.8026\n",
      "iprec_at_recall_0.70  \tall\t0.3816\n",
      "iprec_at_recall_0.80  \tall\t0.1184\n",
      "iprec_at_recall_0.90  \tall\t0.0526\n",
      "iprec_at_recall_1.00  \tall\t0.0263\n",
      "P_5                   \tall\t0.9553\n",
      "P_10                  \tall\t0.8895\n",
      "P_15                  \tall\t0.8114\n",
      "P_20                  \tall\t0.7441\n",
      "P_30                  \tall\t0.6325\n",
      "P_100                 \tall\t0.2733\n",
      "P_200                 \tall\t0.1376\n",
      "P_500                 \tall\t0.0550\n",
      "P_1000                \tall\t0.0275\n"
     ]
    }
   ],
   "source": [
    "!../bin/trec_eval-9.0.7/trec_eval ../data/processed/cisi/qrels.txt ../data/processed/cisi/results_abstract_top10.txt -J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trec_eval [-h] [-q] [-m measure[.params] [-c] [-n] [-l <num>]\n",
      "   [-D debug_level] [-N <num>] [-M <num>] [-R rel_format] [-T results_format]\n",
      "   rel_info_file  results_file \n",
      " \n",
      "Calculate and print various evaluation measures, evaluating the results  \n",
      "in results_file against the relevance info in rel_info_file. \n",
      " \n",
      "There are a fair number of options, of which only the lower case options are \n",
      "normally ever used.   \n",
      " --help:\n",
      " -h: Print full help message and exit. Full help message will include\n",
      "     descriptions for any measures designated by a '-m' parameter, and\n",
      "     input file format descriptions for any rel_info_format given by '-R'\n",
      "     and any top results_format given by '-T.'\n",
      "     Thus to see all info about preference measures use\n",
      "          trec_eval -h -m all_prefs -R prefs -T trec_results \n",
      " --version:\n",
      " -v: Print version of trec_eval and exit.\n",
      " --query_eval_wanted:\n",
      " -q: In addition to summary evaluation, give evaluation for each query/topic\n",
      " --measure measure_name[.measure_params]:\n",
      " -m measure: Add 'measure' to the lists of measures to calculate and print.\n",
      "    If 'measure' contains a '.', then the name of the measure is everything\n",
      "    preceeding the period, and everything to the right of the period is\n",
      "    assumed to be a list of parameters for the measure, separated by ','. \n",
      "    There can be multiple occurrences of the -m flag.\n",
      "    'measure' can also be a nickname for a set of measures. Current \n",
      "    nicknames include \n",
      "       'official': the main measures often used by TREC\n",
      "       'all_trec': all measures calculated with the standard TREC\n",
      "                   results and rel_info format files.\n",
      "       'set': subset of all_trec that calculates unranked values.\n",
      "       'prefs': Measures not in all_trec that calculate preference measures.\n",
      " --complete_rel_info_wanted:\n",
      " -c: Average over the complete set of queries in the relevance judgements  \n",
      "     instead of the queries in the intersection of relevance judgements \n",
      "     and results.  Missing queries will contribute a value of 0 to all \n",
      "     evaluation measures (which may or may not be reasonable for a  \n",
      "     particular evaluation measure, but is reasonable for standard TREC \n",
      "     measures.) Default is off.\n",
      " --level_for_rel num:\n",
      " -l<num>: Num indicates the minimum relevance judgement value needed for \n",
      "      a document to be called relevant. Used if rel_info_file contains \n",
      "      relevance judged on a multi-relevance scale.  Default is 1. \n",
      " --nosummary:\n",
      " -n: No summary evaluation will be printed\n",
      " --Debug_level num:\n",
      " -D <num>: Debug level.  1 and 2 used for measures, 3 and 4 for merging\n",
      "     rel_info and results, 5 and 6 for input.  Currently, num can be of the\n",
      "     form <num>.<qid> and only qid will be evaluated with debug info printed.\n",
      "     Default is 0.\n",
      " --Number_docs_in_coll num:\n",
      " -N <num>: Number of docs in collection Default is MAX_LONG \n",
      " -Max_retrieved_per_topic num:\n",
      " -M <num>: Max number of docs per topic to use in evaluation (discard rest). \n",
      "      Default is MAX_LONG.\n",
      " --Judged_docs_only:\n",
      " -J: Calculate all values only over the judged (either relevant or  \n",
      "     nonrelevant) documents.  All unjudged documents are removed from the \n",
      "     retrieved set before any calculations (possibly leaving an empty set). \n",
      "     DO NOT USE, unless you really know what you're doing - very easy to get \n",
      "     reasonable looking numbers in a file that you will later forget were \n",
      "     calculated  with the -J flag.  \n",
      " --Rel_info_format format:\n",
      " -R format: The rel_info file is assumed to be in format 'format'.  Current\n",
      "    values for 'format' include 'qrels', 'prefs', 'qrels_prefs'.  Note not\n",
      "    all measures can be calculated with all formats.\n",
      " --Results_format format:\n",
      " -T format: the top results_file is assumed to be in format 'format'. Current\n",
      "    values for 'format' include 'trec_results'. Note not all measures can be\n",
      "    calculated with all formats.\n",
      " --Zscore Zmean_file:\n",
      " -Z Zmean_file: Instead of printing the raw score for each measure, print\n",
      "    a Z score instead. The score printed will be the deviation from the mean\n",
      "    of the raw score, expressed in standard deviations, where the mean and\n",
      "    standard deviation for each measure and query are found in Zmean_file.\n",
      "    If mean is not in Zmeanfile for a measure and query, -1000000 is printed.\n",
      "    Zmean_file format is ascii lines of form \n",
      "       qid  measure_name  mean  std_dev\n",
      " \n",
      " \n",
      "Standard evaluation procedure:\n",
      "For each of the standard TREC measures requested, a ranked list of\n",
      "of relevance judgements is created corresponding to each ranked retrieved doc,\n",
      "A rel judgement is set to -1 if the document was not in the pool (not in \n",
      "rel_info_file) or -2 if the document was in the pool but unjudged (some \n",
      "measures (infAP) allow the pool to be sampled instead of judged fully).  \n",
      "Otherwise it is set to the value in rel_info_file. \n",
      "Most measures, but not all, will treat -1 or -2 the same as 0, \n",
      "namely nonrelevant.  Note that relevance_level is used to \n",
      "determine if the document is relevant during score calculations. \n",
      "Queries for which there is no relevance information are ignored. \n",
      "Warning: queries for which there are relevant docs but no retrieved docs \n",
      "are also ignored by default.  This allows systems to evaluate over subsets  \n",
      "of the relevant docs, but means if a system improperly retrieves no docs,  \n",
      "it will not be detected.  Use the -c flag to avoid this behavior. \n",
      "\n",
      "-----------------------\n",
      "Results_file format: Standard 'trec_results'\n",
      "Lines of results_file are of the form \n",
      "     030  Q0  ZF08-175-870  0   4238   prise1 \n",
      "     qid iter   docno      rank  sim   run_id \n",
      "giving TREC document numbers (a string) retrieved by query qid  \n",
      "(a string) with similarity sim (a float).  The other fields are ignored, \n",
      "with the exception that the run_id field of the last line is kept and \n",
      "output.  In particular, note that the rank field is ignored here; \n",
      "internally ranks are assigned by sorting by the sim field with ties  \n",
      "broken deterministicly (using docno). \n",
      "Sim is assumed to be higher for the docs to be retrieved first. \n",
      "File may contain no NULL characters. \n",
      "Lines may contain fields after the run_id; they are ignored. \n",
      "\n",
      "-----------------------\n",
      "Rel_info_file format: Standard 'qrels'\n",
      "Relevance for each docno to qid is determined from rel_info_file, which \n",
      "consists of text tuples of the form \n",
      "   qid  iter  docno  rel \n",
      "giving TREC document numbers (docno, a string) and their relevance (rel,  \n",
      "a non-negative integer less than 128, or -1 (unjudged)) \n",
      "to query qid (a string).  iter string field is ignored.   \n",
      "Fields are separated by whitespace, string fields can contain no whitespace. \n",
      "File may contain no NULL characters. \n",
      "\n",
      "-----------------------\n",
      "Individual measure documentation for requested measures\n",
      "-- No measures indicated.\n",
      "   Request measure documentation using <-m measure> on command line\n"
     ]
    }
   ],
   "source": [
    "!../bin/trec_eval-9.0.7/trec_eval -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f9cff93c564aed0795eb3a2079f10dc3221fa71e77d246f2a568ea3248ec283"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
