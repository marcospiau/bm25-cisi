{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict, namedtuple\n",
    "from pathlib import Path\n",
    "import toolz\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import more_itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dos dados\n",
    "\n",
    "Para fazer download dos dados, vá até a pasta raiz e digite:\n",
    "```bash\n",
    "make get_raw_data\n",
    "\n",
    "```\n",
    "Para gerar esse Makefile, eu dei a seguinte query ao chat GPT:\n",
    "```\n",
    "write a makefile to do the following:\n",
    "- download file from http://ir.dcs.gla.ac.uk/resources/test_collections/cisi/cisi.tar.gz\n",
    "- unzip it to the folder data/raw\n",
    "- it should be on step \"get_raw_data\"\n",
    "- operation should not be repeated if files already exist\n",
    "- write help for the steps\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAW_DATA_BASEDIR = Path('../data/raw')\n",
    "!ls -lht {RAW_DATA_BASEDIR}\n",
    "\n",
    "raw_files_path = {\n",
    "    'query': RAW_DATA_BASEDIR / 'CISI.QRY',\n",
    "    'all': RAW_DATA_BASEDIR / 'CISI.ALL',\n",
    "    'rel': RAW_DATA_BASEDIR / 'CISI.REL',\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversão dos dados para um formato mais fácil de trabalhar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6200\n",
      "-rw-r--r--  1 marcospiau  staff   757K Feb 22 19:27 cisi.tar.gz\n",
      "-rw-r--r--  1 marcospiau  staff    79K Feb 28  1994 CISI.REL\n",
      "-rw-r--r--  1 marcospiau  staff    67K Feb 28  1994 CISI.QRY\n",
      "-rw-r--r--  1 marcospiau  staff   4.5K Feb 28  1994 CISI.BLN\n",
      "-rw-r--r--  1 marcospiau  staff   2.1M Feb 28  1994 CISI.ALL\n"
     ]
    }
   ],
   "source": [
    "RAW_DATA_BASEDIR = Path('../data/raw')\n",
    "!ls -lht {RAW_DATA_BASEDIR}\n",
    "\n",
    "raw_files_path = {\n",
    "    'query': RAW_DATA_BASEDIR / 'CISI.QRY',\n",
    "    'all': RAW_DATA_BASEDIR / 'CISI.ALL',\n",
    "    'rel': RAW_DATA_BASEDIR / 'CISI.REL',\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquivos QRY e ALL são mais complicados e precisam de processamento. Tentei deixar processamento semelhante pra conseguir fazer os dois casos com uma mesma funcao.\n",
    "\n",
    "Usei bastante o chatgpt, principalmente para escrever docstrings e typehints. Uma coisa curiosa é que constantemente ele removia a dataclass e utilizava classes padrão, de forma que precisei constamente instruir ele a a manter as dataclasses.\n",
    "\n",
    "Utilizei bastante a bibliioteca toolz para deixar o código com uma abordagem mais funcional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (.I) ID\n",
    "# (.T) Title\n",
    "# (.W) Abstract\n",
    "# (.B) Publication date of the article\n",
    "# (.A) Author list\n",
    "# (.N) Information when entry was added\n",
    "# (.X) List of cross-references to other documents\n",
    "\n",
    "renames_docs_all = {\n",
    "    '.I': 'id',\n",
    "    '.T': 'title',\n",
    "    '.W': 'abstract',\n",
    "    '.B': 'publication_date',\n",
    "    '.A': 'author_list',\n",
    "    '.N': 'added_date',\n",
    "    '.X': 'cross_references'\n",
    "}\n",
    "\n",
    "process_doc_all = toolz.compose_left(\n",
    "    # for each tag, get renamed tag, and join texts for tag\n",
    "    functools.partial(more_itertools.map_reduce,\n",
    "                      keyfunc=lambda x: renames_docs_all[x.tag],\n",
    "                      valuefunc=lambda x: x.text,\n",
    "                      reducefunc=lambda x: ' '.join(x)),\n",
    "    # keeps only desired keys\n",
    "    toolz.curried.keyfilter(lambda x: x in {'id', 'title', 'abstract'}),\n",
    "    # convert id to int\n",
    "    toolz.curried.update_in(keys=['id'], func=int)\n",
    ")\n",
    "\n",
    "# (.I) ID\n",
    "# (.W) Query\n",
    "# (.A) Author list\n",
    "# (.N) Authors name and some keywords on what the query searches for\n",
    "\n",
    "renames_docs_queries = {\n",
    "    '.I': 'id',\n",
    "    '.W': 'query',\n",
    "    '.T': 'title',\n",
    "    '.A': 'author_list',\n",
    "    # '.N': 'other_query_infos',\n",
    "    '.B': 'publication_date',\n",
    "    # '.X': 'cross_references'\n",
    "}\n",
    "\n",
    "process_doc_qry = toolz.compose_left(\n",
    "    functools.partial(more_itertools.map_reduce,\n",
    "                      keyfunc=lambda x: renames_docs_queries[x.tag],\n",
    "                      valuefunc=lambda x: x.text,\n",
    "                      reducefunc=lambda x: ' '.join(x)),\n",
    "    toolz.curried.keyfilter(lambda x: x in {'id', 'query'}),\n",
    "    toolz.curried.update_in(keys=['id'], func=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "from typing import Any, Callable, Dict, Iterable, List, Tuple, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IdTagText:\n",
    "    id: str\n",
    "    tag: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "def parse_cisi_all_or_qry(\n",
    "    path: str,\n",
    "    process_doc_fn: Callable[[Iterable[IdTagText]], Dict[str, Any]],\n",
    "    return_dict: bool = False,\n",
    ") -> Union[List[Dict[str, Any]], Dict[str, Any]]:\n",
    "    \"\"\"Parses a CISI all or query file.\n",
    "\n",
    "    Args:\n",
    "        path: A string representing the path to the file to parse.\n",
    "        process_doc_fn: A function that receives a list of IdTagText or an iterable of\n",
    "          IdTagText as input, and returns a dictionary.\n",
    "        return_dict: A boolean indicating whether to return a dictionary or a list of\n",
    "          dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        If return_dict is True, a dictionary with the document IDs as keys and\n",
    "        processed documents as values. Otherwise, a list of processed documents.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the document IDs are not unique.\n",
    "    \"\"\"\n",
    "    markers = [r'^\\.I\\s(\\d+)$', '\\.T$', '\\.A$', '\\.B$', '\\.W$', '\\.X$', '\\.N$']\n",
    "    marker_pattern = re.compile('|'.join(markers))\n",
    "\n",
    "    def gen_items() -> Iterable[IdTagText]:\n",
    "        with open(path, 'r') as f:\n",
    "            for line in map(str.strip, f):\n",
    "                match = marker_pattern.match(line)\n",
    "                # match occurs for lines with tags\n",
    "                if match:\n",
    "                    # if match.group(1) is not None, it means that the tag is .I\n",
    "                    # and the group contains the ID\n",
    "                    if match.group(1):\n",
    "                        id_ = match.group(1)\n",
    "                        yield IdTagText(id_, '.I', id_)\n",
    "                    else:\n",
    "                        tag = match.group(0).strip()\n",
    "                else:\n",
    "                    # if match is None, it means that the line is a text\n",
    "                    # just propagate tag and id\n",
    "                    yield IdTagText(id_, tag, line)\n",
    "\n",
    "    out = [\n",
    "        process_doc_fn(group)\n",
    "        for _, group in itertools.groupby(gen_items(), key=lambda x: x.id)\n",
    "    ]\n",
    "    assert len(out) == len({x['id'] for x in out}), 'IDs are not unique'\n",
    "    return {x['id']: x for x in out} if return_dict else out\n",
    "\n",
    "\n",
    "process_cisi_all = functools.partial(parse_cisi_all_or_qry,\n",
    "                                     process_doc_fn=process_doc_all)\n",
    "process_cisi_qry = functools.partial(parse_cisi_all_or_qry,\n",
    "                                     process_doc_fn=process_doc_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'title': '18 Editions of the Dewey Decimal Classifications',\n",
       " 'abstract': \"The present study is a history of the DEWEY Decimal Classification.  The first edition of the DDC was published in 1876, the eighteenth edition in 1971, and future editions will continue to appear as needed.  In spite of the DDC's long and healthy life, however, its full story has never been told.  There have been biographies of Dewey that briefly describe his system, but this is the first attempt to provide a detailed history of the work that more than any other has spurred the growth of librarianship in this country and abroad.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = process_cisi_all(raw_files_path['all'])\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'query': 'What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = process_cisi_qry(raw_files_path['query'])\n",
    "queries[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O arquivo qrels é mais simples e pode ser lido de forma mais simples. O resultado é um dicionário com a query como chave e uma lista de documentos relevantes para aquela query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {28,\n",
       "   35,\n",
       "   38,\n",
       "   42,\n",
       "   43,\n",
       "   52,\n",
       "   65,\n",
       "   76,\n",
       "   86,\n",
       "   150,\n",
       "   189,\n",
       "   192,\n",
       "   193,\n",
       "   195,\n",
       "   215,\n",
       "   269,\n",
       "   291,\n",
       "   320,\n",
       "   429,\n",
       "   465,\n",
       "   466,\n",
       "   482,\n",
       "   483,\n",
       "   510,\n",
       "   524,\n",
       "   541,\n",
       "   576,\n",
       "   582,\n",
       "   589,\n",
       "   603,\n",
       "   650,\n",
       "   680,\n",
       "   711,\n",
       "   722,\n",
       "   726,\n",
       "   783,\n",
       "   813,\n",
       "   820,\n",
       "   868,\n",
       "   869,\n",
       "   894,\n",
       "   1162,\n",
       "   1164,\n",
       "   1195,\n",
       "   1196,\n",
       "   1281}),\n",
       " (2,\n",
       "  {29,\n",
       "   68,\n",
       "   197,\n",
       "   213,\n",
       "   214,\n",
       "   309,\n",
       "   319,\n",
       "   324,\n",
       "   429,\n",
       "   499,\n",
       "   636,\n",
       "   669,\n",
       "   670,\n",
       "   674,\n",
       "   690,\n",
       "   692,\n",
       "   695,\n",
       "   700,\n",
       "   704,\n",
       "   709,\n",
       "   720,\n",
       "   731,\n",
       "   733,\n",
       "   738,\n",
       "   740,\n",
       "   1136}),\n",
       " (3,\n",
       "  {60,\n",
       "   85,\n",
       "   114,\n",
       "   123,\n",
       "   126,\n",
       "   131,\n",
       "   133,\n",
       "   136,\n",
       "   138,\n",
       "   140,\n",
       "   346,\n",
       "   359,\n",
       "   363,\n",
       "   372,\n",
       "   412,\n",
       "   445,\n",
       "   454,\n",
       "   461,\n",
       "   463,\n",
       "   469,\n",
       "   532,\n",
       "   537,\n",
       "   540,\n",
       "   553,\n",
       "   554,\n",
       "   555,\n",
       "   585,\n",
       "   590,\n",
       "   599,\n",
       "   640,\n",
       "   660,\n",
       "   664,\n",
       "   803,\n",
       "   901,\n",
       "   909,\n",
       "   911,\n",
       "   1027,\n",
       "   1053,\n",
       "   1169,\n",
       "   1179,\n",
       "   1181,\n",
       "   1190,\n",
       "   1191,\n",
       "   1326}),\n",
       " (4, {310, 315, 321, 329, 332, 420, 601, 980}),\n",
       " (5,\n",
       "  {32,\n",
       "   56,\n",
       "   65,\n",
       "   114,\n",
       "   124,\n",
       "   137,\n",
       "   187,\n",
       "   188,\n",
       "   191,\n",
       "   241,\n",
       "   339,\n",
       "   370,\n",
       "   400,\n",
       "   451,\n",
       "   453,\n",
       "   471,\n",
       "   525,\n",
       "   528,\n",
       "   642,\n",
       "   648,\n",
       "   692,\n",
       "   1035,\n",
       "   1246,\n",
       "   1356})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_qrels(path):\n",
    "    out = defaultdict(set)\n",
    "    with open(path, 'r') as f:\n",
    "        for line in map(str.strip, f):\n",
    "            qid, docid, _, _ = line.split()\n",
    "            out[int(qid)].add(int(docid))\n",
    "    out.default_factory = None\n",
    "\n",
    "    return out\n",
    "\n",
    "qrels = load_qrels(raw_files_path['rel'])\n",
    "list(itertools.islice(qrels.items(), 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo, vemos que todos os qrels possuem relevância igual a zero. Por isso, vamos interpretar que caso um documento esteja relacionado a uma query, ele é relevante para essa query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docid</th>\n",
       "      <th>nao</th>\n",
       "      <th>sei</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3114</td>\n",
       "      <td>3114</td>\n",
       "      <td>3114</td>\n",
       "      <td>3114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>76</td>\n",
       "      <td>1162</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>44</td>\n",
       "      <td>375</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>155</td>\n",
       "      <td>15</td>\n",
       "      <td>3114</td>\n",
       "      <td>3114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         qid docid   nao   sei\n",
       "count   3114  3114  3114  3114\n",
       "unique    76  1162     1     1\n",
       "top       44   375     0   0.0\n",
       "freq     155    15  3114  3114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(raw_files_path['rel'], delim_whitespace=True, header=None, names=['qid', 'docid', 'nao', 'sei']).astype(str)\n",
    "display(df.describe())\n",
    "del df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe BM25\n",
    "\n",
    "Utilizei bastante o ChatGPT para desevolver essa classe, comecei com uma classe mais simples (loops for) e fui modificando para conseguir deixar um pouco mais vetorizado. Estamos usando o CountVectorizer para facilitar a tokenização dos documentos e queries, o que também ajuda pois estão incluídos os stop words.\n",
    "\n",
    "Novamente, docstrings e typehints foram gerados pelo chatgpt.\n",
    "\n",
    "Utilizamos numpy e matrizes esparsas scipy para facilitar vetorização e operações matriciais.\n",
    "\n",
    "Valores default de k1 e b foram escolhidos pelo ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class BM25:\n",
    "\n",
    "    def __init__(self, corpus: List[dict], k1: float = 1.2, b: float = 0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "\n",
    "        self.corpus = {doc['id']: doc['text'] for doc in corpus}\n",
    "        self.docids_to_idx = {\n",
    "            docid: idx\n",
    "            for idx, docid in enumerate(self.corpus.keys())\n",
    "        }\n",
    "\n",
    "        # self.vectorizer = CountVectorizer(lowercase=True,\n",
    "        #                                   max_features=None,\n",
    "        #                                   stop_words=None,\n",
    "        #                                   tokenizer=self.tokenize,\n",
    "        #                                   preprocessor=lambda x: x)\n",
    "        self.vectorizer = CountVectorizer(lowercase=True,\n",
    "                                          max_features=None,\n",
    "                                          stop_words='english')\n",
    "        self.doc_term_matrix = self.vectorizer.fit_transform(\n",
    "            self.corpus.values())\n",
    "        self.corpus_lengths = self.doc_term_matrix.sum(axis=1).A1\n",
    "        self.avgdl = self.corpus_lengths.mean()\n",
    "        self.idf = self._calculate_idf()\n",
    "        # self.idf = self._calculate_idf()\n",
    "        # self.avgdl = sum(map(len, (doc['text'] for doc in self.corpus))) / len(self.corpus)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        raise NotImplementedError\n",
    "        # return re.sub(r'[^\\w\\s]', '', text).lower().split()\n",
    "\n",
    "\n",
    "    def _calculate_idf(self) -> np.ndarray:\n",
    "        \"\"\"Calculate the inverse document frequency for each term in the corpus using the BM25 formula.\"\"\"\n",
    "        # documents with term\n",
    "        df = self.doc_term_matrix.getnnz(axis=0)\n",
    "        N = self.doc_term_matrix.shape[0]\n",
    "        idf = np.log((N - df + 0.5) / (df + 0.5))\n",
    "        return idf\n",
    "\n",
    "    def score(self, query: str, doc_id: int) -> float:\n",
    "        \"\"\"Calculate the relevance score of a document for a given query.\n",
    "\n",
    "        Args:\n",
    "            query: Query text.\n",
    "            doc_id: ID of the document in the corpus.\n",
    "\n",
    "        Returns:\n",
    "            Relevance score of the document for the query.\n",
    "        \"\"\"\n",
    "        doc_tf = self.doc_term_matrix.getrow(self.docids_to_idx[doc_id])\n",
    "        doc_length = doc_tf.sum()\n",
    "        query_tf = self.vectorizer.transform([query])\n",
    "        score = 0\n",
    "        for weight, term in zip(query_tf.data, query_tf.indices):\n",
    "            idf = self.idf[term]\n",
    "            tf = doc_tf[0, term]\n",
    "            score += idf * weight * (self.k1 + 1) / (\n",
    "                tf + self.k1 * (1 - self.b + self.b * doc_length / self.avgdl))\n",
    "        return score\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 10) -> List[dict]:\n",
    "        \"\"\"Retrieve the top-k documents for a given query.\n",
    "\n",
    "        Args:\n",
    "            query: Query text.\n",
    "            k: Number of documents to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            List of document dictionaries with 'id' and 'score' keys.\n",
    "        \"\"\"\n",
    "        scores = [\n",
    "            self.score(query, doc_id)\n",
    "            for doc_id in self.corpus.keys()\n",
    "        ]\n",
    "        docids = np.argsort(scores)[::-1][:k]\n",
    "        return [\n",
    "            {'id': docid, 'score': scores[docid]}\n",
    "            for docid in docids\n",
    "        ]\n",
    "\n",
    "    def get_results_for_all_queries(self, queries: List[dict], k: int = 10) -> Dict[int, List[dict]]:\n",
    "        \"\"\"Retrieve the top-k documents for all queries.\n",
    "\n",
    "        Args:\n",
    "            queries: List of query dictionaries with 'id' and 'text' keys.\n",
    "            k: Number of documents to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping query IDs to a list of document dictionaries with 'id' and 'score' keys.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            query['id']: self.retrieve(query['query'], k)\n",
    "            for query in tqdm(queries, desc='Retrieving')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_abstracts = [{'id': x['id'], 'text': x['abstract']} for x in corpus]\n",
    "bm25 = BM25(corpus=corpus_abstracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175.0623179775004"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.score(query=corpus[0]['abstract'], doc_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1287, 'score': 366.937721998151},\n",
       " {'id': 1295, 'score': 366.937721998151},\n",
       " {'id': 1283, 'score': 353.04613444162817},\n",
       " {'id': 1085, 'score': 340.1679993689292},\n",
       " {'id': 1301, 'score': 340.1679993689292},\n",
       " {'id': 1288, 'score': 340.1679993689292},\n",
       " {'id': 930, 'score': 340.1679993689292},\n",
       " {'id': 1311, 'score': 340.1679993689292},\n",
       " {'id': 1300, 'score': 328.1963161321295},\n",
       " {'id': 1278, 'score': 328.1963161321295}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.retrieve(query=queries[0]['query'], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving: 100%|██████████| 112/112 [00:59<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "results_top10 = bm25.get_results_for_all_queries(queries, k=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliacao das métricas\n",
    "\n",
    "Para facilitar avaliacão das métricas e ter um script confiável, vamos utilizar o script trec_eval. Ele está instalado na pasta `bin`, dentro da raiz do projeto."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertendo dados para format trec_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QRELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 1281 1\n",
      "1 0 650 1\n",
      "1 0 1162 1\n",
      "1 0 524 1\n",
      "1 0 269 1\n",
      "1 0 1164 1\n",
      "1 0 783 1\n",
      "1 0 894 1\n",
      "1 0 150 1\n",
      "1 0 28 1\n"
     ]
    }
   ],
   "source": [
    "from typing import Set\n",
    "\n",
    "def convert_qrels_to_trec_format(qrels: Dict[int, Set[int]], output_path: str):\n",
    "    \"\"\"Convert qrels to TREC format.\n",
    "\n",
    "    Args:\n",
    "        qrels: Dictionary of qrels.\n",
    "        output_path: Path to the output file.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        for qid, docids in qrels.items():\n",
    "            for docid in docids:\n",
    "                f.write(f'{qid} 0 {docid} 1\\n')\n",
    "\n",
    "!mkdir -pv '../data/processed/cisi'\n",
    "convert_qrels_to_trec_format(qrels, '../data/processed/cisi/qrels.txt')\n",
    "!head '../data/processed/cisi/qrels.txt'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1287, 'score': 366.937721998151},\n",
       " {'id': 1295, 'score': 366.937721998151},\n",
       " {'id': 1283, 'score': 353.04613444162817},\n",
       " {'id': 1085, 'score': 340.1679993689292},\n",
       " {'id': 1301, 'score': 340.1679993689292},\n",
       " {'id': 1288, 'score': 340.1679993689292},\n",
       " {'id': 930, 'score': 340.1679993689292},\n",
       " {'id': 1311, 'score': 340.1679993689292},\n",
       " {'id': 1300, 'score': 328.1963161321295},\n",
       " {'id': 1278, 'score': 328.1963161321295}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_top10[1] = [{'id': 1287, 'score': 366.937721998151},\n",
    " {'id': 1295, 'score': 366.937721998151},\n",
    " {'id': 1283, 'score': 353.04613444162817},\n",
    " {'id': 1085, 'score': 340.1679993689292},\n",
    " {'id': 1301, 'score': 340.1679993689292},\n",
    " {'id': 1288, 'score': 340.1679993689292},\n",
    " {'id': 930, 'score': 340.1679993689292},\n",
    " {'id': 1311, 'score': 340.1679993689292},\n",
    " {'id': 1300, 'score': 328.1963161321295},\n",
    " {'id': 1278, 'score': 328.1963161321295}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Q0 1287 1 366.937721998151 RUN\n",
      "1 Q0 1295 2 366.937721998151 RUN\n",
      "1 Q0 1283 3 353.04613444162817 RUN\n",
      "1 Q0 1085 4 340.1679993689292 RUN\n",
      "1 Q0 1301 5 340.1679993689292 RUN\n",
      "1 Q0 1288 6 340.1679993689292 RUN\n",
      "1 Q0 930 7 340.1679993689292 RUN\n",
      "1 Q0 1311 8 340.1679993689292 RUN\n",
      "1 Q0 1300 9 328.1963161321295 RUN\n",
      "1 Q0 1278 10 328.1963161321295 RUN\n"
     ]
    }
   ],
   "source": [
    "def convert_results_to_trec_eval_format(results: Dict[str, List[str]], output_path: str):\n",
    "    \"\"\"Converts results to trec_eval format.\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary of results.\n",
    "        output_path: Path to the output file.\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        for query_id, docs in results.items():\n",
    "            for rank, doc in enumerate(docs, start=1):\n",
    "                doc_id = doc['id']\n",
    "                score = doc['score']\n",
    "                line = f\"{query_id} Q0 {doc_id} {rank} {score} RUN\\n\"\n",
    "                f.write(line)\n",
    "\n",
    "convert_results_to_trec_eval_format(results_top10, '../data/processed/cisi/results_abstract_top10.txt')\n",
    "!head '../data/processed/cisi/results_abstract_top10.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f9cff93c564aed0795eb3a2079f10dc3221fa71e77d246f2a568ea3248ec283"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
